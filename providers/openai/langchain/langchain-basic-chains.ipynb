{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1bd8f30-2e64-4ac7-9d6c-2612852f85a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jcolamendy/python/tutorials/nlp-tutorials/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20209daa-fde7-4508-94f5-967fdae447fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import load_dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ed99d6-e0f3-437b-be45-f38dcf69a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "190425a6-385d-4d09-afe7-48b04ee90461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Deep learning is a subset of machine learning that uses artificial neural networks to model and solve complex problems. It is a type of machine learning that involves algorithms that are capable of learning and making decisions on their own, without the need for human intervention. Deep learning is commonly used in areas such as image and speech recognition, natural language processing, and computer vision.', response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 13, 'total_tokens': 84}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-85588d0d-f44f-410c-a7d1-e71225c17b45-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is an deep learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c82b614-99e2-4f3d-ae77-e5353916fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "    (\"user\", \"{text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23749b78-43e1-4d4a-a05e-c232de893779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "messages=[SystemMessage(content='You are a helpful assistant that translates English to French.'), HumanMessage(content='I love programming.')]\n"
     ]
    }
   ],
   "source": [
    "output = prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee655c9b-e4fb-45ed-a095-e7533928a7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"J'adore l'apprentissage profond.\" response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 27, 'total_tokens': 39}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None} id='run-fc95980e-1c79-4632-81e8-b4e979d5bc4a-0'\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "output = chain.invoke({\"input_language\": \"English\", \"output_language\": \"French\", \"text\": \"I love deep learning.\"})\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ac0cd25-8981-4e25-992e-36e1e52b437c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "J'adore la programmation.\n"
     ]
    }
   ],
   "source": [
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "output = chain.invoke({\"input_language\": \"English\", \"output_language\": \"French\", \"text\": \"I love programming.\"})\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e4481510-549a-4aa1-a552-95dfc827c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'input_language': 'English', 'output_language': 'French', 'text': \"J'adore l'apprentissage profond.\"}\n"
     ]
    }
   ],
   "source": [
    "output_parser = StrOutputParser()\n",
    "chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n",
    "\n",
    "output = chain.invoke({\"input_language\": \"English\", \"output_language\": \"French\", \"text\": \"I love deep learning.\"})\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87904ba5-ce30-405a-bdd3-879b1ebcc6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='Rainbow Soles' response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 22, 'total_tokens': 26}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None} id='run-c6aef605-5710-4cca-96ed-1d940d1be3ea-0'\n",
      "<class 'str'>\n",
      "\n",
      "\n",
      "\"Rainbow Socks Co.\" or \"Spectrum Socks Inc.\"\n"
     ]
    }
   ],
   "source": [
    "# llm vs chat models\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = OpenAI()\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "\n",
    "output = chat_model.invoke(messages)\n",
    "print(type(output))\n",
    "print(output)\n",
    "\n",
    "output = llm.invoke(text)\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8729fb3-285d-4a0a-ab3b-7b0e4d704771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "What is a good name for a company that makes colorful socks?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "output = prompt.format(product=\"colorful socks\")\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3374c59e-660b-4df5-9743-13a3edfe25eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "messages=[SystemMessage(content='You are a helpful assistant that translates English to French.'), HumanMessage(content='I love programming.')]\n",
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to French.'), HumanMessage(content='I love programming.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "output = chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
    "print(type(output))\n",
    "print(output)\n",
    "\n",
    "output = chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e829a35-a596-4818-9380-2328da1bc33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'bye']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "output_parser.parse(\"hi, bye\")\n",
    "# >> ['hi', 'bye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f2bd2f3-9d43-47de-bd6a-4b790d89f74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue', 'red', 'green', 'yellow', 'purple']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"Generate a list of 5 {text}.\\n\\n{format_instructions}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(template)\n",
    "chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions())\n",
    "chain = chat_prompt | chat_model | output_parser\n",
    "chain.invoke({\"text\": \"colors\"})\n",
    "# >> ['red', 'blue', 'green', 'yellow', 'orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa81fd8f-ef63-4712-810f-3b19db4b3bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"), HumanMessage(content=\"I don't like eating tasty things\")]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that re-writes the user's text to \"\n",
    "                \"sound more upbeat.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "messages = chat_template.format_messages(text=\"I don't like eating tasty things\")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e065496d-fd4c-476c-80fb-bf6dfc31d1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the best way to learn programming?'),\n",
       " AIMessage(content='1. Choose a programming language: Decide on a programming language that you want to learn.\\n\\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\\n\\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience'),\n",
       " HumanMessage(content='Summarize our conversation so far in 10 words.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "human_prompt = \"Summarize our conversation so far in {word_count} words.\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"conversation\"), \n",
    "        HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "    ]\n",
    ")\n",
    "\n",
    "human_message = HumanMessage(content=\"What is the best way to learn programming?\")\n",
    "ai_message = AIMessage(\n",
    "    content=\"\"\"\\\n",
    "1. Choose a programming language: Decide on a programming language that you want to learn.\n",
    "\n",
    "2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\n",
    "\n",
    "3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chat_prompt.format_prompt(\n",
    "    conversation=[human_message, ai_message], word_count=\"10\"\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07ced4a6-d7a5-422b-a4cf-50c68d8beb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Tell me a funny joke about chickens.'\n",
      "Tell me a funny joke about chickens.\n",
      "[HumanMessage(content='Tell me a funny joke about chickens.')]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"Tell me a {adjective} joke about {content}.\")\n",
    "\n",
    "output = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})\n",
    "print(output)\n",
    "# StringPromptValue(text='Tell me a funny joke about chickens.')\n",
    "\n",
    "output = prompt_val.to_string()\n",
    "print(output)\n",
    "#'Tell me a funny joke about chickens.'\n",
    "\n",
    "output = prompt_val.to_messages()\n",
    "print(output)\n",
    "#[HumanMessage(content='Tell me a funny joke about chickens.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d42dbc97-e4be-43b3-9cc3-dcf1bdad8462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "messages=[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"), HumanMessage(content='i dont like eating tasty things.')]\n",
      "[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"), HumanMessage(content='i dont like eating tasty things.')]\n",
      "System: You are a helpful assistant that re-writes the user's text to sound more upbeat.\n",
      "Human: i dont like eating tasty things.\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that re-writes the user's text to \"\n",
    "                \"sound more upbeat.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_val = chat_template.invoke({\"text\": \"i dont like eating tasty things.\"})\n",
    "print(type(chat_val))\n",
    "print(chat_val)\n",
    "\n",
    "output = chat_val.to_messages()\n",
    "print(output)\n",
    "\n",
    "output = chat_val.to_string()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca1bc98e-cf69-410a-a471-6fe85d84ff61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foobaz\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{foo}{bar}\")\n",
    "partial_prompt = prompt.partial(foo=\"foo\")\n",
    "print(partial_prompt.format(bar=\"baz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec79cc53-9e7d-434d-896a-6ddc0e3b0288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about the day 04/05/2024, 00:14:29\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def _get_datetime():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "    input_variables=[\"adjective\", \"date\"],\n",
    ")\n",
    "partial_prompt = prompt.partial(date=_get_datetime)\n",
    "print(partial_prompt.format(adjective=\"funny\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f77b264d-ac10-4ad7-acc6-2927111ea3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "J'adore la programmation.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "output = chain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming\")\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc6e2aa4-2f0c-4a30-9042-56dcb9908021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'input_language': 'English', 'output_language': 'French', 'text': \"J'adore la programmation.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "output = chain.invoke({\"input_language\": \"English\", \"output_language\": \"French\", \"text\": \"I love programming.\"})\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbfc9dd6-2cd1-42b6-aaf8-85b0ce3c0d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model learns the training data too well, including noise and random fluctuations, which can lead to poor performance on new, unseen data. Regularization techniques add a penalty term to the model's loss function to discourage overly complex models that may fit the training data too closely. This helps to improve the model's generalization ability and make it more effective at making predictions on new data. Regularization techniques help strike a balance between model complexity and performance, leading to more robust and accurate models.\", response_metadata={'token_usage': {'completion_tokens': 118, 'prompt_tokens': 24, 'total_tokens': 142}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-c39b00d9-7643-4b63-aa8e-7f088647eddf-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"What is the purpose of model regularization?\"),\n",
    "]\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "058f2055-62f3-4209-a209-d43cbbd87a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of model regularization is to prevent a machine learning model from overfitting the training data. Overfitting occurs when a model learns the training data too well and performs poorly on new, unseen data. Regularization techniques help to control the complexity of the model by adding a penalty term to the loss function, which discourages overly complex models that may be fitting noise in the data rather than the underlying patterns. This helps improve the model's generalization performance on unseen data and makes it more robust. Common regularization techniques include L1 and L2 regularization, dropout, and early stopping."
     ]
    }
   ],
   "source": [
    "for chunk in chat.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1d64fc-2ccb-4345-83de-70b6e88ce8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some people may find this joke funny, while others may not. It ultimately depends on individual sense of humor.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = llm = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "\n",
    "joke_chain = prompt | model | StrOutputParser()\n",
    "# joke_chain.invoke({\"topic\": \"bears\"})\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
    "analysis_chain = analysis_prompt | model | StrOutputParser()\n",
    "\n",
    "composed_chain = {\"joke\": joke_chain} | analysis_chain\n",
    "\n",
    "# call\n",
    "composed_chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6477fe8-dae4-4675-bd1e-3b2e838f0de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, this joke is funny! It plays on the literal meaning of \"root\" as well as the common phrase \"root of the problem.\"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way\n",
    "composed_chain_with_lambda = (\n",
    "    joke_chain\n",
    "    | (lambda input: {\"joke\": input})\n",
    "    | analysis_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# call\n",
    "composed_chain_with_lambda.invoke({\"topic\": \"beets\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a634d39-0c28-44bc-b380-e0d49cb43a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some people may find this joke funny, especially if they are familiar with the TV show Battlestar Galactica and the relationship between Cylons and toasters. However, humor is subjective so not everyone may find it funny.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "composed_chain_runnable = (\n",
    "    RunnableParallel({\"joke\": joke_chain})\n",
    "    | analysis_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# call\n",
    "composed_chain_runnable.invoke({\"topic\": \"battlestar galactica\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b40f72ea-686e-439b-a959-7e1e2a227451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Jesse loves red but not yellow'), Document(page_content='Jamal loves green but not as much as he loves orange')]\n",
      "<class 'list'>\n",
      "input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='You are an AI Assistant.\\n\\nYour task is to answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))]\n",
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "first={\n",
      "  context: VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x107993a90>),\n",
      "  question: RunnablePassthrough()\n",
      "} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='You are an AI Assistant.\\n\\nYour task is to answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))]), ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10ee94fa0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10fa65430>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')] last=StrOutputParser()\n",
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# docs\n",
    "docs = [\n",
    "    Document(page_content=\"Jesse loves red but not yellow\"),\n",
    "    Document(page_content = \"Jamal loves green but not as much as he loves orange\")\n",
    "]\n",
    "print(docs)\n",
    "print(type(docs))\n",
    "\n",
    "# embed docs into vector store\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=OpenAIEmbeddings())\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# prompt\n",
    "template = \"\"\"You are an AI Assistant.\n",
    "\n",
    "Your task is to answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(prompt)\n",
    "print(type(prompt))\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chain_input = {\"context\": retriever , \"question\": RunnablePassthrough()}\n",
    "rag_chain = (\n",
    "    chain_input\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# call\n",
    "rag_chain.invoke(\"What does Jesse love?\")\n",
    "\n",
    "print(rag_chain)\n",
    "print(type(rag_chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d3fa12b-61ab-4478-bfe9-f5f1815c8c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first={\n",
      "  context: RunnableLambda(itemgetter('question'))\n",
      "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x107993a90>)\n",
      "           | RunnableLambda(format_docs),\n",
      "  question: RunnableLambda(itemgetter('question'))\n",
      "} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='You are an AI Assistant.\\n\\nYour task is to answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))]), ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10ee94fa0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10fa65430>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')] last=StrOutputParser()\n",
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Jesse loves red.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chain_input = {\n",
    "    \"context\": itemgetter(\"question\") | retriever | format_docs, \n",
    "    #\"question\": RunnablePassthrough()\n",
    "    \"question\": itemgetter(\"question\")\n",
    "}\n",
    "rag_chain = (\n",
    "    chain_input\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(rag_chain)\n",
    "print(type(rag_chain))\n",
    "\n",
    "# call\n",
    "rag_chain.invoke({\"question\": \"What does Jesse love?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9947378-24ca-44a9-a657-2b03164b150c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Jamal loves green but not as much as he loves orange'),\n",
       " Document(page_content='Jamal loves green but not as much as he loves orange'),\n",
       " Document(page_content='Jesse loves red but not yellow'),\n",
       " Document(page_content='Jesse loves red but not yellow')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke('What does Jamal love?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "385a71ff-150d-4a20-87b4-573cb83412c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesse's favorite color is red, while Jamal's favorite color is orange, which he loves more than green. Jesse does not like yellow, and Jamal does not love green as much as he loves orange.\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# prompt\n",
    "qa_system_prompt = \"\"\"You are an AI assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"human\", \"Question: {input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# call\n",
    "docs = [\n",
    "    Document(page_content=\"Jesse loves red but not yellow\"),\n",
    "    Document(page_content = \"Jamal loves green but not as much as he loves orange\")\n",
    "]\n",
    "question = \"What are everyone's favorite colors?\"\n",
    "ai_msg_1 = qa_chain.invoke({\"input\": question, \"context\": docs})\n",
    "print(ai_msg_1)\n",
    "print(type(ai_msg_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5778937e-8dfd-4ef8-b2c9-00833f1341d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant for question-answering tasks.\n",
      "Your task is answer a question given the provided context by following the instructions below.\n",
      "\n",
      "% INSTRUCTIONS:\n",
      "You must follow the instructions:\n",
      "- only use the context to answer the question\n",
      "- if you don't know or find the answer, just say that you don't know\n",
      "- use three sentences maximum for the answer\n",
      "- keep the answer concise\n",
      "\n",
      "% CONTEXT:\n",
      "{context}\n",
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "input_variables=['context', 'input'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an AI assistant for question-answering tasks.\\nYour task is answer a question given the provided context by following the instructions below.\\n\\n% INSTRUCTIONS:\\nYou must follow the instructions:\\n- only use the context to answer the question\\n- if you don't know or find the answer, just say that you don't know\\n- use three sentences maximum for the answer\\n- keep the answer concise\\n\\n% CONTEXT:\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='Question: {input}'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "## create_stuff_documents_chain chain\n",
    "# prompt\n",
    "qa_system_prompt = \"\"\"You are an AI assistant for question-answering tasks.\n",
    "Your task is answer a question given the provided context by following the instructions below.\n",
    "\n",
    "% INSTRUCTIONS:\n",
    "You must follow the instructions:\n",
    "- only use the context to answer the question\n",
    "- if you don't know or find the answer, just say that you don't know\n",
    "- use three sentences maximum for the answer\n",
    "- keep the answer concise\n",
    "\n",
    "% CONTEXT:\n",
    "{context}\"\"\"\n",
    "\n",
    "print(qa_system_prompt)\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"human\", \"Question: {input}\"),\n",
    "    ]\n",
    ")\n",
    "print(type(qa_prompt))\n",
    "print(qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54b9831f-18f9-45ed-9f7c-87bf0727bd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableBinding'>\n",
      "bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "  context: RunnableLambda(format_docs)\n",
      "}), config={'run_name': 'format_inputs'})\n",
      "| ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an AI assistant for question-answering tasks.\\nYour task is answer a question given the provided context by following the instructions below.\\n\\n% INSTRUCTIONS:\\nYou must follow the instructions:\\n- only use the context to answer the question\\n- if you don't know or find the answer, just say that you don't know\\n- use three sentences maximum for the answer\\n- keep the answer concise\\n\\n% CONTEXT:\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='Question: {input}'))])\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1297e2970>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1297dfe80>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
      "| StrOutputParser() config={'run_name': 'stuff_documents_chain'}\n"
     ]
    }
   ],
   "source": [
    "# llm\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# build create_stuff_documents_chain chain\n",
    "qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "print(type(qa_chain))\n",
    "print(qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "088773d3-38d9-490c-89a6-8928262174dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## retriever\n",
    "# docs\n",
    "docs = [\n",
    "    Document(page_content=\"Jesse loves red but not yellow\"),\n",
    "    Document(page_content = \"Jamal loves green but not as much as he loves orange\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcafad83-e757-403a-8ae3-2938095e0651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesse's favorite color is red, while Jamal's favorite color is orange. Jesse doesn't like yellow, and Jamal doesn't like green as much as he likes orange.\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "output = qa_chain.invoke({\n",
    "    \"context\": docs,\n",
    "    \"input\": \"What are everyone's favorite colors?\"\n",
    "})\n",
    "print(output)\n",
    "print(type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97711af4-d3a0-4409-aae4-2beb1b8a3998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.vectorstores.VectorStoreRetriever'>\n"
     ]
    }
   ],
   "source": [
    "# embed docs into vector store\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=OpenAIEmbeddings())\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(type(retriever))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c9731c9-8958-4c3d-9671-1abf964a69d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableBinding'>\n",
      "bound=RunnableAssign(mapper={\n",
      "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
      "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x10ecba400>), config={'run_name': 'retrieve_documents'})\n",
      "})\n",
      "| RunnableAssign(mapper={\n",
      "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "              context: RunnableLambda(format_docs)\n",
      "            }), config={'run_name': 'format_inputs'})\n",
      "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an AI assistant for question-answering tasks.\\nYour task is answer a question given the provided context by following the instructions below.\\n\\n% INSTRUCTIONS:\\nYou must follow the instructions:\\n- only use the context to answer the question\\n- if you don't know or find the answer, just say that you don't know\\n- use three sentences maximum for the answer\\n- keep the answer concise\\n\\n% CONTEXT:\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='Question: {input}'))])\n",
      "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1297e2970>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1297dfe80>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
      "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
      "  }) config={'run_name': 'retrieval_chain'}\n"
     ]
    }
   ],
   "source": [
    "## retrieval_chain chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, qa_chain)\n",
    "print(type(retrieval_chain))\n",
    "print(retrieval_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3230f282-4d5c-4932-a814-418119080092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"What are everyone's favorite colors?\",\n",
       " 'context': [Document(page_content='Jesse loves red but not yellow'),\n",
       "  Document(page_content='Jesse loves red but not yellow'),\n",
       "  Document(page_content='Jesse loves red but not yellow'),\n",
       "  Document(page_content='Jamal loves green but not as much as he loves orange')],\n",
       " 'answer': \"Jesse's favorite color is red, and Jamal's favorite color is orange. Jesse does not like yellow, and Jamal does not like green as much as he likes orange.\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke({\"input\": \"What are everyone's favorite colors?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66e2ae0-cc0f-426e-8ab6-42b493fb04be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "first=ChatPromptTemplate(input_variables=['input_language', 'output_language', 'text'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], template='You are a helpful assistant that translates {input_language} to {output_language}.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))]) last=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x108ef0bb0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x108ef8640>, openai_api_key=SecretStr('**********'), openai_proxy='')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "print(type(chain))\n",
    "print(chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
