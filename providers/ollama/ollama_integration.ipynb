{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8b95c8-e9f3-47b7-9b81-28a0de8432b4",
   "metadata": {},
   "source": [
    "# Pull llama3 using ollama\n",
    "```sh\n",
    "ollama pull llama3\n",
    "```\n",
    "\n",
    "## Prompts\n",
    "- https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa93be9-12d6-4092-b019-b1a2ac980a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "158ba8e3-a04a-4f1c-abcb-c0203d151afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LLM\n",
    "local_llm = 'llama3'\n",
    "llm = ChatOllama(model=local_llm, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2c3e968-0644-4ce2-a77b-46b8844b4ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
      "input_variables=['context', 'question'] template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an AI assistant for question-answering tasks.\\nGiven the context below, your task is to answer the question using the instructions below.\\n\\n% INSTRUCTIONS:\\nFollow the instructions to answer the question:\\n- only use the context to answer the question\\n- if you don't know the answer, just say [I don't know]\\n- use three sentences maximum and keep the answer concise\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n% CONTEXT:\\n{context}\\n\\n% QUESTION: \\n{question} \\n\\n% ANSWER:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n"
     ]
    }
   ],
   "source": [
    "# define the prompt\n",
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an AI assistant for question-answering tasks.\n",
    "Given the context below, your task is to answer the question using the instructions below.\n",
    "\n",
    "% INSTRUCTIONS:\n",
    "Follow the instructions to answer the question:\n",
    "- only use the context to answer the question\n",
    "- if you don't know the answer, just say [I don't know]\n",
    "- use three sentences maximum and keep the answer concise\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "% CONTEXT:\n",
    "{context}\n",
    "\n",
    "% QUESTION: \n",
    "{question} \n",
    "\n",
    "% ANSWER:\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "prompt = prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "794952dc-8db5-47ef-9c8a-c84d5079524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=PromptTemplate(input_variables=['context', 'question'], template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an AI assistant for question-answering tasks.\\nGiven the context below, your task is to answer the question using the instructions below.\\n\\n% INSTRUCTIONS:\\nFollow the instructions to answer the question:\\n- only use the context to answer the question\\n- if you don't know the answer, just say [I don't know]\\n- use three sentences maximum and keep the answer concise\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n% CONTEXT:\\n{context}\\n\\n% QUESTION: \\n{question} \\n\\n% ANSWER:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\") middle=[ChatOllama(model='llama3', temperature=0.0)] last=StrOutputParser()\n",
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# define the qa chain\n",
    "qa_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(qa_chain)\n",
    "print(type(qa_chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87fbf27c-3fbc-4df2-a4d1-d97e421e1310",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Ollama is a user-friendly interface for running large language models (LLMs) locally, specifically on MacOS and Linux, with Windows support on the horizon. It is a valuable tool for researchers, developers, and anyone who wants to experiment with language models. Ollama supports a wide range of models, including Lama 2, Lama 2 uncensored, and the newly released Mistal 7B, among others.\n",
    "\n",
    "Features\n",
    "- Ease of Use: Ollama is easy to install and use, even for users with no prior experience with language models. It provides a simple API for creating, running, and managing models.\n",
    "- Versatility and Model Installation: Ollama supports a wide range of models, making it versatile for various applications. It also provides a straightforward installation process, making it appealing to individuals and small teams.\n",
    "- GPU Acceleration: Ollama leverages GPU acceleration, which can speed up model inference by up to 2x compared to CPU-only setups. This feature is particularly beneficial for tasks that require heavy computation. This feature is included out of the box and it requires zero intervention.\n",
    "- Integration Capabilities: Ollama is compatible with several platforms like Langchain, llama-index, and more.\n",
    "- Privacy and Cost: Running LLMs locally with Ollama ensures data privacy as your data is not sent to a third party. It also eliminates inference fees, which is important for token-intensive applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7c6e638-e5b8-4378-a37c-c25c1e10861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is a user-friendly interface for running large language models (LLMs) locally on MacOS and Linux, with Windows support coming soon. It provides an easy-to-use API for creating, running, and managing models, making it accessible to researchers, developers, and anyone interested in experimenting with language models.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is ollama\"\n",
    "output = qa_chain.invoke({\"question\": question, \"context\": context})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2dace3f-a0b2-44c7-8634-44fd56342b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can run Ollama on your GPU. According to the context, Ollama leverages GPU acceleration, which can speed up model inference by up to 2x compared to CPU-only setups. This feature is included out of the box and requires zero intervention.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can I run ollama on my GPU\"\n",
    "output = qa_chain.invoke({\"question\": question, \"context\": context})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28908fab-b183-41e8-93fe-47b1a6a122da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The context does not mention \"Llama3\". It only mentions Lama 2 and Mistal 7B as specific language models supported by Ollama.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is llama3\"\n",
    "output = qa_chain.invoke({\"question\": question, \"context\": context})\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
