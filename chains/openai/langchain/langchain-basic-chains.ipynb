{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1bd8f30-2e64-4ac7-9d6c-2612852f85a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jcolamendy/python/tutorials/nlp-tutorials/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20209daa-fde7-4508-94f5-967fdae447fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import load_dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ed99d6-e0f3-437b-be45-f38dcf69a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "190425a6-385d-4d09-afe7-48b04ee90461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Deep learning is a subset of machine learning that uses artificial neural networks to model and solve complex problems. It is a type of machine learning that involves algorithms that are capable of learning and making decisions on their own, without the need for human intervention. Deep learning is commonly used in areas such as image and speech recognition, natural language processing, and computer vision.', response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 13, 'total_tokens': 84}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-85588d0d-f44f-410c-a7d1-e71225c17b45-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is an deep learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c82b614-99e2-4f3d-ae77-e5353916fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "    (\"user\", \"{text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23749b78-43e1-4d4a-a05e-c232de893779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "messages=[SystemMessage(content='You are a helpful assistant that translates English to French.'), HumanMessage(content='I love programming.')]\n"
     ]
    }
   ],
   "source": [
    "output = prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee655c9b-e4fb-45ed-a095-e7533928a7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"J'adore l'apprentissage profond.\" response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 27, 'total_tokens': 39}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None} id='run-fc95980e-1c79-4632-81e8-b4e979d5bc4a-0'\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "output = chain.invoke({\"input_language\": \"English\", \"output_language\": \"French\", \"text\": \"I love deep learning.\"})\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ac0cd25-8981-4e25-992e-36e1e52b437c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "J'adore la programmation.\n"
     ]
    }
   ],
   "source": [
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "output = chain.invoke({\"input_language\": \"English\", \"output_language\": \"French\", \"text\": \"I love programming.\"})\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e4481510-549a-4aa1-a552-95dfc827c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'input_language': 'English', 'output_language': 'French', 'text': \"J'adore l'apprentissage profond.\"}\n"
     ]
    }
   ],
   "source": [
    "output_parser = StrOutputParser()\n",
    "chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n",
    "\n",
    "output = chain.invoke({\"input_language\": \"English\", \"output_language\": \"French\", \"text\": \"I love deep learning.\"})\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87904ba5-ce30-405a-bdd3-879b1ebcc6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='Rainbow Soles' response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 22, 'total_tokens': 26}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None} id='run-c6aef605-5710-4cca-96ed-1d940d1be3ea-0'\n",
      "<class 'str'>\n",
      "\n",
      "\n",
      "\"Rainbow Socks Co.\" or \"Spectrum Socks Inc.\"\n"
     ]
    }
   ],
   "source": [
    "# llm vs chat models\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = OpenAI()\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "\n",
    "output = chat_model.invoke(messages)\n",
    "print(type(output))\n",
    "print(output)\n",
    "\n",
    "output = llm.invoke(text)\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8729fb3-285d-4a0a-ab3b-7b0e4d704771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "What is a good name for a company that makes colorful socks?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "output = prompt.format(product=\"colorful socks\")\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3374c59e-660b-4df5-9743-13a3edfe25eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "messages=[SystemMessage(content='You are a helpful assistant that translates English to French.'), HumanMessage(content='I love programming.')]\n",
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to French.'), HumanMessage(content='I love programming.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "output = chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
    "print(type(output))\n",
    "print(output)\n",
    "\n",
    "output = chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e829a35-a596-4818-9380-2328da1bc33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'bye']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "output_parser.parse(\"hi, bye\")\n",
    "# >> ['hi', 'bye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f2bd2f3-9d43-47de-bd6a-4b790d89f74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue', 'red', 'green', 'yellow', 'purple']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"Generate a list of 5 {text}.\\n\\n{format_instructions}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(template)\n",
    "chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions())\n",
    "chain = chat_prompt | chat_model | output_parser\n",
    "chain.invoke({\"text\": \"colors\"})\n",
    "# >> ['red', 'blue', 'green', 'yellow', 'orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa81fd8f-ef63-4712-810f-3b19db4b3bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"), HumanMessage(content=\"I don't like eating tasty things\")]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that re-writes the user's text to \"\n",
    "                \"sound more upbeat.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "messages = chat_template.format_messages(text=\"I don't like eating tasty things\")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e065496d-fd4c-476c-80fb-bf6dfc31d1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the best way to learn programming?'),\n",
       " AIMessage(content='1. Choose a programming language: Decide on a programming language that you want to learn.\\n\\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\\n\\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience'),\n",
       " HumanMessage(content='Summarize our conversation so far in 10 words.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "human_prompt = \"Summarize our conversation so far in {word_count} words.\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"conversation\"), \n",
    "        HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "    ]\n",
    ")\n",
    "\n",
    "human_message = HumanMessage(content=\"What is the best way to learn programming?\")\n",
    "ai_message = AIMessage(\n",
    "    content=\"\"\"\\\n",
    "1. Choose a programming language: Decide on a programming language that you want to learn.\n",
    "\n",
    "2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\n",
    "\n",
    "3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chat_prompt.format_prompt(\n",
    "    conversation=[human_message, ai_message], word_count=\"10\"\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07ced4a6-d7a5-422b-a4cf-50c68d8beb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Tell me a funny joke about chickens.'\n",
      "Tell me a funny joke about chickens.\n",
      "[HumanMessage(content='Tell me a funny joke about chickens.')]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"Tell me a {adjective} joke about {content}.\")\n",
    "\n",
    "output = prompt_template.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})\n",
    "print(output)\n",
    "# StringPromptValue(text='Tell me a funny joke about chickens.')\n",
    "\n",
    "output = prompt_val.to_string()\n",
    "print(output)\n",
    "#'Tell me a funny joke about chickens.'\n",
    "\n",
    "output = prompt_val.to_messages()\n",
    "print(output)\n",
    "#[HumanMessage(content='Tell me a funny joke about chickens.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d42dbc97-e4be-43b3-9cc3-dcf1bdad8462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "messages=[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"), HumanMessage(content='i dont like eating tasty things.')]\n",
      "[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"), HumanMessage(content='i dont like eating tasty things.')]\n",
      "System: You are a helpful assistant that re-writes the user's text to sound more upbeat.\n",
      "Human: i dont like eating tasty things.\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that re-writes the user's text to \"\n",
    "                \"sound more upbeat.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_val = chat_template.invoke({\"text\": \"i dont like eating tasty things.\"})\n",
    "print(type(chat_val))\n",
    "print(chat_val)\n",
    "\n",
    "output = chat_val.to_messages()\n",
    "print(output)\n",
    "\n",
    "output = chat_val.to_string()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca1bc98e-cf69-410a-a471-6fe85d84ff61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foobaz\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{foo}{bar}\")\n",
    "partial_prompt = prompt.partial(foo=\"foo\")\n",
    "print(partial_prompt.format(bar=\"baz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec79cc53-9e7d-434d-896a-6ddc0e3b0288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about the day 04/05/2024, 00:14:29\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def _get_datetime():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "    input_variables=[\"adjective\", \"date\"],\n",
    ")\n",
    "partial_prompt = prompt.partial(date=_get_datetime)\n",
    "print(partial_prompt.format(adjective=\"funny\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f77b264d-ac10-4ad7-acc6-2927111ea3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "J'adore la programmation.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "output = chain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming\")\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc6e2aa4-2f0c-4a30-9042-56dcb9908021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'input_language': 'English', 'output_language': 'French', 'text': \"J'adore la programmation.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "output = chain.invoke({\"input_language\": \"English\", \"output_language\": \"French\", \"text\": \"I love programming.\"})\n",
    "print(type(output))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbfc9dd6-2cd1-42b6-aaf8-85b0ce3c0d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The purpose of model regularization is to prevent overfitting in machine learning models. Overfitting occurs when a model learns the training data too well, including noise and random fluctuations, which can lead to poor performance on new, unseen data. Regularization techniques add a penalty term to the model's loss function to discourage overly complex models that may fit the training data too closely. This helps to improve the model's generalization ability and make it more effective at making predictions on new data. Regularization techniques help strike a balance between model complexity and performance, leading to more robust and accurate models.\", response_metadata={'token_usage': {'completion_tokens': 118, 'prompt_tokens': 24, 'total_tokens': 142}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-c39b00d9-7643-4b63-aa8e-7f088647eddf-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"What is the purpose of model regularization?\"),\n",
    "]\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "058f2055-62f3-4209-a209-d43cbbd87a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of model regularization is to prevent a machine learning model from overfitting the training data. Overfitting occurs when a model learns the training data too well and performs poorly on new, unseen data. Regularization techniques help to control the complexity of the model by adding a penalty term to the loss function, which discourages overly complex models that may be fitting noise in the data rather than the underlying patterns. This helps improve the model's generalization performance on unseen data and makes it more robust. Common regularization techniques include L1 and L2 regularization, dropout, and early stopping."
     ]
    }
   ],
   "source": [
    "for chunk in chat.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1d64fc-2ccb-4345-83de-70b6e88ce8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some people may find this joke funny, while others may not. It ultimately depends on individual sense of humor.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = llm = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "\n",
    "joke_chain = prompt | model | StrOutputParser()\n",
    "# joke_chain.invoke({\"topic\": \"bears\"})\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
    "analysis_chain = analysis_prompt | model | StrOutputParser()\n",
    "\n",
    "composed_chain = {\"joke\": joke_chain} | analysis_chain\n",
    "\n",
    "# call\n",
    "composed_chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6477fe8-dae4-4675-bd1e-3b2e838f0de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, this joke is funny! It plays on the literal meaning of \"root\" as well as the common phrase \"root of the problem.\"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way\n",
    "composed_chain_with_lambda = (\n",
    "    joke_chain\n",
    "    | (lambda input: {\"joke\": input})\n",
    "    | analysis_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# call\n",
    "composed_chain_with_lambda.invoke({\"topic\": \"beets\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a634d39-0c28-44bc-b380-e0d49cb43a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some people may find this joke funny, especially if they are familiar with the TV show Battlestar Galactica and the relationship between Cylons and toasters. However, humor is subjective so not everyone may find it funny.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "composed_chain_runnable = (\n",
    "    RunnableParallel({\"joke\": joke_chain})\n",
    "    | analysis_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# call\n",
    "composed_chain_runnable.invoke({\"topic\": \"battlestar galactica\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b40f72ea-686e-439b-a959-7e1e2a227451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Jesse loves red but not yellow'), Document(page_content='Jamal loves green but not as much as he loves orange')]\n",
      "<class 'list'>\n",
      "input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='You are an AI Assistant.\\n\\nYour task is to answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))]\n",
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "first={\n",
      "  context: VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x107993a90>),\n",
      "  question: RunnablePassthrough()\n",
      "} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='You are an AI Assistant.\\n\\nYour task is to answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))]), ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10ee94fa0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10fa65430>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')] last=StrOutputParser()\n",
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# docs\n",
    "docs = [\n",
    "    Document(page_content=\"Jesse loves red but not yellow\"),\n",
    "    Document(page_content = \"Jamal loves green but not as much as he loves orange\")\n",
    "]\n",
    "print(docs)\n",
    "print(type(docs))\n",
    "\n",
    "# embed docs into vector store\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=OpenAIEmbeddings())\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# prompt\n",
    "template = \"\"\"You are an AI Assistant.\n",
    "\n",
    "Your task is to answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(prompt)\n",
    "print(type(prompt))\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chain_input = {\"context\": retriever , \"question\": RunnablePassthrough()}\n",
    "rag_chain = (\n",
    "    chain_input\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# call\n",
    "rag_chain.invoke(\"What does Jesse love?\")\n",
    "\n",
    "print(rag_chain)\n",
    "print(type(rag_chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d3fa12b-61ab-4478-bfe9-f5f1815c8c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first={\n",
      "  context: RunnableLambda(itemgetter('question'))\n",
      "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x107993a90>)\n",
      "           | RunnableLambda(format_docs),\n",
      "  question: RunnableLambda(itemgetter('question'))\n",
      "} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='You are an AI Assistant.\\n\\nYour task is to answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'))]), ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10ee94fa0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10fa65430>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')] last=StrOutputParser()\n",
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Jesse loves red.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chain_input = {\n",
    "    \"context\": itemgetter(\"question\") | retriever | format_docs, \n",
    "    #\"question\": RunnablePassthrough()\n",
    "    \"question\": itemgetter(\"question\")\n",
    "}\n",
    "rag_chain = (\n",
    "    chain_input\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(rag_chain)\n",
    "print(type(rag_chain))\n",
    "\n",
    "# call\n",
    "rag_chain.invoke({\"question\": \"What does Jesse love?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9947378-24ca-44a9-a657-2b03164b150c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Jamal loves green but not as much as he loves orange'),\n",
       " Document(page_content='Jamal loves green but not as much as he loves orange'),\n",
       " Document(page_content='Jesse loves red but not yellow'),\n",
       " Document(page_content='Jesse loves red but not yellow')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke('What does Jamal love?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "385a71ff-150d-4a20-87b4-573cb83412c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesse's favorite color is red, while Jamal's favorite color is orange, which he loves more than green. Jesse does not like yellow, and Jamal does not love green as much as he loves orange.\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# prompt\n",
    "qa_system_prompt = \"\"\"You are an AI assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"human\", \"Question: {input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# call\n",
    "docs = [\n",
    "    Document(page_content=\"Jesse loves red but not yellow\"),\n",
    "    Document(page_content = \"Jamal loves green but not as much as he loves orange\")\n",
    "]\n",
    "question = \"What are everyone's favorite colors?\"\n",
    "ai_msg_1 = qa_chain.invoke({\"input\": question, \"context\": docs})\n",
    "print(ai_msg_1)\n",
    "print(type(ai_msg_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5778937e-8dfd-4ef8-b2c9-00833f1341d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant for question-answering tasks.\n",
      "Your task is answer a question given the provided context by following the instructions below.\n",
      "\n",
      "% INSTRUCTIONS:\n",
      "You must follow the instructions:\n",
      "- only use the context to answer the question\n",
      "- if you don't know or find the answer, just say that you don't know\n",
      "- use three sentences maximum for the answer\n",
      "- keep the answer concise\n",
      "\n",
      "% CONTEXT:\n",
      "{context}\n",
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "input_variables=['context', 'input'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an AI assistant for question-answering tasks.\\nYour task is answer a question given the provided context by following the instructions below.\\n\\n% INSTRUCTIONS:\\nYou must follow the instructions:\\n- only use the context to answer the question\\n- if you don't know or find the answer, just say that you don't know\\n- use three sentences maximum for the answer\\n- keep the answer concise\\n\\n% CONTEXT:\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='Question: {input}'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "## create_stuff_documents_chain chain\n",
    "# prompt\n",
    "qa_system_prompt = \"\"\"You are an AI assistant for question-answering tasks.\n",
    "Your task is answer a question given the provided context by following the instructions below.\n",
    "\n",
    "% INSTRUCTIONS:\n",
    "You must follow the instructions:\n",
    "- only use the context to answer the question\n",
    "- if you don't know or find the answer, just say that you don't know\n",
    "- use three sentences maximum for the answer\n",
    "- keep the answer concise\n",
    "\n",
    "% CONTEXT:\n",
    "{context}\"\"\"\n",
    "\n",
    "print(qa_system_prompt)\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"human\", \"Question: {input}\"),\n",
    "    ]\n",
    ")\n",
    "print(type(qa_prompt))\n",
    "print(qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54b9831f-18f9-45ed-9f7c-87bf0727bd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableBinding'>\n",
      "bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "  context: RunnableLambda(format_docs)\n",
      "}), config={'run_name': 'format_inputs'})\n",
      "| ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an AI assistant for question-answering tasks.\\nYour task is answer a question given the provided context by following the instructions below.\\n\\n% INSTRUCTIONS:\\nYou must follow the instructions:\\n- only use the context to answer the question\\n- if you don't know or find the answer, just say that you don't know\\n- use three sentences maximum for the answer\\n- keep the answer concise\\n\\n% CONTEXT:\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='Question: {input}'))])\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1297e2970>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1297dfe80>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
      "| StrOutputParser() config={'run_name': 'stuff_documents_chain'}\n"
     ]
    }
   ],
   "source": [
    "# llm\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# build create_stuff_documents_chain chain\n",
    "qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "print(type(qa_chain))\n",
    "print(qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "088773d3-38d9-490c-89a6-8928262174dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## retriever\n",
    "# docs\n",
    "docs = [\n",
    "    Document(page_content=\"Jesse loves red but not yellow\"),\n",
    "    Document(page_content = \"Jamal loves green but not as much as he loves orange\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcafad83-e757-403a-8ae3-2938095e0651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesse's favorite color is red, while Jamal's favorite color is orange. Jesse doesn't like yellow, and Jamal doesn't like green as much as he likes orange.\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "output = qa_chain.invoke({\n",
    "    \"context\": docs,\n",
    "    \"input\": \"What are everyone's favorite colors?\"\n",
    "})\n",
    "print(output)\n",
    "print(type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97711af4-d3a0-4409-aae4-2beb1b8a3998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.vectorstores.VectorStoreRetriever'>\n"
     ]
    }
   ],
   "source": [
    "# embed docs into vector store\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=OpenAIEmbeddings())\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(type(retriever))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c9731c9-8958-4c3d-9671-1abf964a69d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableBinding'>\n",
      "bound=RunnableAssign(mapper={\n",
      "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
      "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x10ecba400>), config={'run_name': 'retrieve_documents'})\n",
      "})\n",
      "| RunnableAssign(mapper={\n",
      "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "              context: RunnableLambda(format_docs)\n",
      "            }), config={'run_name': 'format_inputs'})\n",
      "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"You are an AI assistant for question-answering tasks.\\nYour task is answer a question given the provided context by following the instructions below.\\n\\n% INSTRUCTIONS:\\nYou must follow the instructions:\\n- only use the context to answer the question\\n- if you don't know or find the answer, just say that you don't know\\n- use three sentences maximum for the answer\\n- keep the answer concise\\n\\n% CONTEXT:\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='Question: {input}'))])\n",
      "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1297e2970>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1297dfe80>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
      "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
      "  }) config={'run_name': 'retrieval_chain'}\n"
     ]
    }
   ],
   "source": [
    "## retrieval_chain chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, qa_chain)\n",
    "print(type(retrieval_chain))\n",
    "print(retrieval_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3230f282-4d5c-4932-a814-418119080092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"What are everyone's favorite colors?\",\n",
       " 'context': [Document(page_content='Jesse loves red but not yellow'),\n",
       "  Document(page_content='Jesse loves red but not yellow'),\n",
       "  Document(page_content='Jesse loves red but not yellow'),\n",
       "  Document(page_content='Jamal loves green but not as much as he loves orange')],\n",
       " 'answer': \"Jesse's favorite color is red, and Jamal's favorite color is orange. Jesse does not like yellow, and Jamal does not like green as much as he likes orange.\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke({\"input\": \"What are everyone's favorite colors?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66e2ae0-cc0f-426e-8ab6-42b493fb04be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "first=ChatPromptTemplate(input_variables=['input_language', 'output_language', 'text'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], template='You are a helpful assistant that translates {input_language} to {output_language}.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))]) last=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x108ef0bb0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x108ef8640>, openai_api_key=SecretStr('**********'), openai_proxy='')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "print(type(chain))\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94a4fdae-6c54-4a95-84ae-130b7366ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "\\\n",
    "We believe AI's short—to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which we merge neural AI (such as an LLM) with semi-traditional software.\n",
    "\n",
    "With agents, we allow LLMs to integrate with code — allowing AI to search the web, perform math, and essentially integrate into anything we can build with code. It should be clear the scope of use cases is phenomenal where AI can integrate with the broader world of software.\n",
    "\n",
    "In this introduction to AI agents, we will cover the essential concepts that make them what they are and why that will make them the core of real-world AI in the years to come.\n",
    "\n",
    "---\n",
    "\n",
    "## Neuro-Symbolic Systems\n",
    "\n",
    "Neuro-symbolic systems consist of both neural and symbolic computation, where:\n",
    "\n",
    "- Neural refers to LLMs, embedding models, or other neural network-based models.\n",
    "- Symbolic refers to logic containing symbolic logic, such as code.\n",
    "\n",
    "Both neural and symbolic AI originate from the early philosophical approaches to AI: connectionism (now neural) and symbolism. Symbolic AI is the more traditional AI. Diehard symbolists believed they could achieve true AGI via written rules, ontologies, and other logical functions.\n",
    "\n",
    "The other camp were the connectionists. Connectionism emerged in 1943 with a theoretical neural circuit but truly kicked off with Rosenblatt's perceptron paper in 1958 [1][2]. Both of these approaches to AI are fascinating but deserve more time than we can give them here, so we will leave further exploration of these concepts for a future chapter.\n",
    "\n",
    "Most important to us is understanding where symbolic logic outperforms neural-based compute and vice-versa.\n",
    "\n",
    "| Neural | Symbolic |\n",
    "| --- | --- |\n",
    "| Flexible, learned logic that can cover a huge range of potential scenarios. | Mostly hand-written rules which can be very granular and fine-tuned but hard to scale. |\n",
    "| Hard to interpret why a neural system does what it does. Very difficult or even impossible to predict behavior. | Rules are written and can be understood. When unsure why a particular ouput was produced we can look at the rules / logic to understand. |\n",
    "| Requires huge amount of data and compute to train state-of-the-art neural models, making it hard to add new abilities or update with new information. | Code is relatively cheap to write, it can be updated with new features easily, and latest information can often be added often instantaneously. |\n",
    "| When trained on broad datasets can often lack performance when exposed to unique scenarios that are not well represented in the training data. | Easily customized to unique scenarios. |\n",
    "| Struggles with complex computations such as mathematical operations. | Perform complex computations very quickly and accurately. |\n",
    "\n",
    "Pure neural architectures struggle with many seemingly simple tasks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fddd569c-6b98-4cdc-86e1-c340a1a42a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article_title': '\"Bridging Minds: The Rise of Neuro-Symbolic AI Agents\"'}\n",
      "content='\"Bridging Minds: The Rise of Neuro-Symbolic AI Agents\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 675, 'total_tokens': 692, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'id': 'chatcmpl-BDHrCRvLAGP6n8InXtI7iN5uTItWy', 'finish_reason': 'stop', 'logprobs': None} id='run-e5b3ee78-e855-457f-85d6-e9c638a07984-0' usage_metadata={'input_tokens': 675, 'output_tokens': 17, 'total_tokens': 692, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_model = \"gpt-4o-mini\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model=openai_model)\n",
    "\n",
    "# Defining the system prompt (how the AI should act)\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are an AI assistant that helps generate articles.\"\"\"\n",
    ")\n",
    "\n",
    "# the user prompt is provided by the user, in this case however the only dynamic\n",
    "# input is the article\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "% GOAL\n",
    "You are tasked with creating a name for a article.\n",
    "% INSTRUCTIONS\n",
    "- The name should be based of the context of the article.\n",
    "- Be creative, but make sure the names are clear, catchy, and relevant to the theme of the article.\n",
    "- Only output the article name, no other explanation or text can be provided.\n",
    "% ARTICLE: {article}\"\"\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])\n",
    "\n",
    "# chain\n",
    "title_chain = (\n",
    "    {\"article\": lambda x: x[\"article\"]}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | {\"article_title\": lambda x: x.content}\n",
    ")\n",
    "article_title = title_chain.invoke({\"article\": article})\n",
    "print(article_title)\n",
    "\n",
    "# raw chain\n",
    "raw_title_chain = (\n",
    "    {\"article\": lambda x: x[\"article\"]}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "article_title_msg = raw_title_chain.invoke({\"article\": article})\n",
    "print(article_title_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b67f983d-9d96-4a4c-8bdc-858eadb4f021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'Explore the transformative potential of neuro-symbolic AI agents in \"Bridging Minds: The Rise of Neuro-Symbolic AI Agents.\" This article delves into the integration of neural and symbolic systems, highlighting how these innovative agents can revolutionize real-world applications by merging the flexibility of neural networks with the precision of symbolic logic. Discover the essential concepts behind neuro-symbolic systems, their advantages, and the future of AI as we navigate the evolving landscape of artificial intelligence. Join us in understanding why these agents are poised to become the cornerstone of AI development in the coming years.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "%GOAL\n",
    "You are tasked with creating a description for the article and article title given as context.\n",
    "% INSTRUCTIONS:\n",
    "- Output the SEO friendly article description. Do not output anything other than the description.\n",
    "% CONTEXT:\n",
    "The article is here for you to examine:\n",
    "---\n",
    "{article}\n",
    "---\n",
    "\n",
    "Here is the article title '{article_title}'.\"\"\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])\n",
    "\n",
    "# chain\n",
    "description_chain = (\n",
    "    {\n",
    "        \"article\": lambda x: x[\"article\"],\n",
    "        \"article_title\": lambda x: x[\"article_title\"]\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | {\"summary\": lambda x: x.content}\n",
    ")\n",
    "\n",
    "article_description = description_chain.invoke({\n",
    "    \"article\": article,\n",
    "    \"article_title\": article_title[\"article_title\"]\n",
    "})\n",
    "article_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1d5c3bf-18c4-40ee-9876-07c7c366d3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_paragraph': 'Pure neural architectures struggle with many seemingly simple tasks.',\n",
       " 'edited_paragraph': 'Pure neural architectures often encounter difficulties with tasks that may appear straightforward at first glance.',\n",
       " 'feedback': \"The original sentence is clear but could benefit from a slight rephrasing for improved clarity and flow. By using 'often encounter difficulties' instead of 'struggle,' the sentence becomes more formal and precise. Additionally, 'seemingly simple tasks' can be rephrased to 'tasks that may appear straightforward at first glance' to enhance readability and provide a smoother transition.\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Paragraph(BaseModel):\n",
    "    original_paragraph: str = Field(description=\"The original paragraph\")\n",
    "    edited_paragraph: str = Field(description=\"The improved edited paragraph\")\n",
    "    feedback: str = Field(description=(\n",
    "        \"Constructive feedback on the original paragraph\"\n",
    "    ))\n",
    "\n",
    "openai_model = \"gpt-4o-mini\"\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI(temperature=0.0, model=openai_model)\n",
    "\n",
    "# system prompt\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"%GOAL: You are an AI assistant that helps generate articles.\"\"\"\n",
    ")\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"%TASK:\n",
    "You are tasked with creating a new paragraph for the article given as context. \n",
    "Follow the instructions below.\n",
    "\n",
    "% INSTRUCTIONS:\n",
    "- Choose one paragraph to review and edit. During your edit ensure you provide constructive feedback to the user so theycan learn where to improve their own writing.\n",
    "- Provide the output in `json` format following the schema below\\n{format_instructions}\n",
    "\n",
    "% CONTEXT:\n",
    "The article is here for you to examine:\n",
    "---\n",
    "{article}\n",
    "---\n",
    "\"\"\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Paragraph)\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt]).partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = (\n",
    "    {\"article\": lambda x: x[\"article\"]}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    "    | {\n",
    "        \"original_paragraph\": lambda x: x.original_paragraph,\n",
    "        \"edited_paragraph\": lambda x: x.edited_paragraph,\n",
    "        \"feedback\": lambda x: x.feedback\n",
    "    }\n",
    ")\n",
    "\n",
    "chain.invoke({'article': article})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b76be5d-8245-40aa-a582-4c6aad79ceed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_paragraph': \"Neuro-symbolic systems consist of both neural and symbolic computation, where: - Neural refers to LLMs, embedding models, or other neural network-based models. - Symbolic refers to logic containing symbolic logic, such as code. Both neural and symbolic AI originate from the early philosophical approaches to AI: connectionism (now neural) and symbolism. Symbolic AI is the more traditional AI. Diehard symbolists believed they could achieve true AGI via written rules, ontologies, and other logical functions. The other camp were the connectionists. Connectionism emerged in 1943 with a theoretical neural circuit but truly kicked off with Rosenblatt's perceptron paper in 1958 [1][2]. Both of these approaches to AI are fascinating but deserve more time than we can give them here, so we will leave further exploration of these concepts for a future chapter. Most important to us is understanding where symbolic logic outperforms neural-based compute and vice-versa.\",\n",
       " 'edited_paragraph': \"Neuro-symbolic systems integrate both neural and symbolic computation, where 'neural' encompasses LLMs, embedding models, and other neural network-based architectures, while 'symbolic' pertains to logic that includes symbolic logic, such as programming code. Both neural and symbolic AI trace their roots back to early philosophical approaches: connectionism (now represented by neural networks) and symbolism. Symbolic AI, the more traditional approach, is grounded in the belief of diehard symbolists that true AGI could be achieved through meticulously crafted rules, ontologies, and logical functions. In contrast, connectionism, which began with a theoretical neural circuit in 1943, gained momentum with Rosenblatt's perceptron paper in 1958 [1][2]. While both approaches are intriguing, they warrant more in-depth discussion than we can provide in this chapter. Therefore, we will focus on understanding the contexts in which symbolic logic excels compared to neural-based computation and vice versa.\",\n",
       " 'feedback': \"The original paragraph provides a solid overview of neuro-symbolic systems, but it could benefit from improved clarity and flow. For instance, breaking up long sentences and using more straightforward language can enhance readability. Additionally, consider using transitions to connect ideas more smoothly, which helps guide the reader through the concepts. Lastly, avoid phrases like 'deserve more time than we can give them here' as they can detract from the focus of the paragraph. Instead, emphasize the importance of the upcoming discussion on the strengths of each approach.\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Paragraph(BaseModel):\n",
    "    original_paragraph: str = Field(description=\"The original paragraph\")\n",
    "    edited_paragraph: str = Field(description=\"The improved edited paragraph\")\n",
    "    feedback: str = Field(description=(\n",
    "        \"Constructive feedback on the original paragraph\"\n",
    "    ))\n",
    "\n",
    "openai_model = \"gpt-4o-mini\"\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI(temperature=0.0, model=openai_model)\n",
    "structured_llm = llm.with_structured_output(Paragraph)\n",
    "\n",
    "# system prompt\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"%GOAL: You are an AI assistant that helps generate articles.\"\"\"\n",
    ")\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"%TASK:\n",
    "You are tasked with creating a new paragraph for the article given as context. \n",
    "Follow the instructions below.\n",
    "\n",
    "% INSTRUCTIONS:\n",
    "- Choose one paragraph to review and edit. During your edit ensure you provide constructive feedback to the user so theycan learn where to improve their own writing.\n",
    "\n",
    "% CONTEXT:\n",
    "The article is here for you to examine:\n",
    "---\n",
    "{article}\n",
    "---\n",
    "\"\"\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])\n",
    "\n",
    "chain = (\n",
    "    {\"article\": lambda x: x[\"article\"]}\n",
    "    | prompt\n",
    "    | structured_llm\n",
    "    | {\n",
    "        \"original_paragraph\": lambda x: x.original_paragraph,\n",
    "        \"edited_paragraph\": lambda x: x.edited_paragraph,\n",
    "        \"feedback\": lambda x: x.feedback\n",
    "    }\n",
    ")\n",
    "\n",
    "chain.invoke({'article': article})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0af0efb9-171d-4e3e-83a0-ee02e7c85a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```json\\n{\\n  \"original_paragraph\": \"Pure neural architectures struggle with many seemingly simple tasks.\",\\n  \"edited_paragraph\": \"Pure neural architectures often encounter difficulties with tasks that may appear straightforward at first glance.\",\\n  \"feedback\": \"The original sentence is clear but could benefit from a slight rephrasing to enhance its readability and flow. By using \\'often encounter difficulties\\' instead of \\'struggle,\\' the sentence becomes more formal and precise. Additionally, \\'tasks that may appear straightforward at first glance\\' adds a layer of nuance, suggesting that the simplicity of the task is subjective. Consider varying your sentence structure and using more descriptive language to engage the reader further.\"\\n}\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 138, 'prompt_tokens': 911, 'total_tokens': 1049, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b8bc95a0ac', 'id': 'chatcmpl-BDIR2a6GhKIDmujt1hDoDYhWDMqhh', 'finish_reason': 'stop', 'logprobs': None} id='run-c642fa4d-6c91-4c69-bfbe-265281e7c60d-0' usage_metadata={'input_tokens': 911, 'output_tokens': 138, 'total_tokens': 1049, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"%GOAL: You are an AI assistant that helps generate articles.\"\"\"\n",
    ")\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"%TASK:\n",
    "You are tasked with creating a new paragraph for the article given as context. \n",
    "Follow the instructions below.\n",
    "\n",
    "% INSTRUCTIONS:\n",
    "- Choose one paragraph to review and edit. During your edit ensure you provide constructive feedback to the user so theycan learn where to improve their own writing.\n",
    "- Provide the output in `json` format following the schema below\\n{format_instructions}\n",
    "\n",
    "% CONTEXT:\n",
    "The article is here for you to examine:\n",
    "---\n",
    "{article}\n",
    "---\n",
    "\"\"\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Paragraph)\n",
    "prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt]).partial(format_instructions=parser.get_format_instructions())\n",
    "chain = (\n",
    "    {\"article\": lambda x: x[\"article\"]}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "msg = chain.invoke({\"article\": article})\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7a24579-bc6f-40cb-9b2d-8854f893a7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "original_paragraph='Pure neural architectures struggle with many seemingly simple tasks.' edited_paragraph='Pure neural architectures often encounter difficulties with tasks that may appear straightforward at first glance.' feedback=\"The original sentence is clear but could benefit from a slight rephrasing to enhance its readability and flow. By using 'often encounter difficulties' instead of 'struggle,' the sentence becomes more formal and precise. Additionally, 'tasks that may appear straightforward at first glance' adds a layer of nuance, suggesting that the simplicity of the task is subjective. Consider varying your sentence structure and using more descriptive language to engage the reader further.\"\n"
     ]
    }
   ],
   "source": [
    "print(type(msg))\n",
    "out = parser.invoke(msg.content)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
